{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPUZoFm1DksBG6QG/y53ms",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddfulaa/Machine-Learning-Notes/blob/main/Kernel_Trick.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding the Kernel Trick\n",
        "\n",
        "The Kernel trick is a powerful tool in the field of Machine Learning that enables complex computations to be made on datasets with high dimensions. This technique is particularly useful when working with Support Vector Machines.\n",
        "\n",
        "To understand the Kernel trick, we need to start with the idea that some datasets cannot be easily separated by a linear function in their original space. However, it is possible to transform these datasets into a higher dimensional space where it becomes possible to separate them linearly. \n"
      ],
      "metadata": {
        "id": "7WJxZL0AecS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d8/Kernel_yontemi_ile_veriyi_daha_fazla_dimensiyonlu_uzaya_tasima_islemi.png/800px-Kernel_yontemi_ile_veriyi_daha_fazla_dimensiyonlu_uzaya_tasima_islemi.png\">"
      ],
      "metadata": {
        "id": "Gd-4rUdyMU_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, this process of transformation can be computationally expensive or even impossible, especially when working with large datasets.\n",
        "\n",
        "This is where the Kernel trick comes in handy. It allows us to avoid the computational burden of performing the explicit transformation by using kernel functions. These kernel functions measure the similarity between two points in a higher dimensional space, without actually transforming the entire dataset into that space.\n",
        "\n",
        "### Linear Regression\n",
        "\n",
        "Let's remember the linear regression problem. Suppose we are given an input dataset $X \\in \\mathbb{R}^{n\\times d}$, which contains $n$ rows and $d$ features, and its corresponding values vector $Y \\in \\mathbb{R}^n$. Our goal is to find an optimal vector $\\omega \\in \\mathbb{R}^n$ that best predicts the values in $Y$ based on the features in $X$.\n",
        "\n",
        "We assume that the relationship between $X$ and $Y$ is linear and can be expressed as $y = \\omega^T x$, where $y \\in \\mathbb{R}$ is the predicted value for a given input $x\\in \\mathbb{R}^n$, and $\\omega^T$ is the transpose of the weight vector $\\omega$.\n",
        "\n",
        "To find the optimal weight vector $\\omega$, we use the method of least squares, which involves minimizing the sum of the squared errors between the predicted and actual values of $Y$. Mathematically, we can express this as follows:\n",
        "\n",
        "$$\\omega^* = \\arg\\min_\\omega J(\\omega)=\\arg \\min_\\omega \\sum_{i=1}^n (y_i - \\omega^T x^{(i)}),$$\n",
        "\n",
        "where $x^{(i)}$ is the ith column of $X$ and $y_i$ is the ith element of $Y$. The function $J(\\omega)$ is the sum of the squared errors between the predicted and actual values of $Y$ for a given weight vector $\\omega$. The weight vector $\\omega^*$ that minimizes $J(\\omega)$ is the optimal weight vector that best predicts the values of $Y$ based on the features in $X$.\n",
        "\n",
        "## My data is not linear\n",
        "\n",
        "Linear regression is a powerful tool for finding a linear relationship between variables. However, for complex datasets where non-linear relationships exist, more advanced models are necessary to better represent the data. In certain scenarios, it is still important to preserve the interpretability and simplicity of a linear model.\n",
        "\n",
        "One way to maintain linearity while capturing non-linear relationships is by changing the model to find a relationship expressed as $y=\\omega^T \\phi(x)$, where $\\phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^m$ and $\\omega\\in \\mathbb{R}^m$. By performing ridge regression, we can minimize the least square sum of errors while also regularizing the coefficients, which helps to prevent overfitting:\n",
        "\n",
        "$$\\omega^* = \\arg\\min_\\omega J(\\omega)=\\arg \\min_\\omega \\sum_{i=1}^n (y_i - \\omega^T \\phi(x^{(i)}))^2 + \\lambda\\sum_{j=1}^m\\omega_j^2.$$\n",
        "\n",
        "Here, $\\lambda$ is a hyperparameter that controls the strength of the regularization. By introducing the basis function $\\phi(x)$, we can transform the input features and capture non-linear relationships while still maintaining the interpretability and simplicity of a linear model.\n",
        "\n",
        "If we vectorize $J(\\omega)$, we get $$J(\\omega) = (Y- \\Phi \\omega)^T(Y- \\Phi \\omega) + \\lambda \\omega^T \\omega$$\n",
        "\n",
        "where $\\Phi$ is the following $n\\times d$ matrix\n",
        "$$\\Phi=  \\begin{bmatrix}\n",
        "\\phi(x^{(1)}) \\\\ \\phi(x^{(2)}) \\\\ \\cdots \\\\ \\phi(x^{(n)})\n",
        "\\end{bmatrix}.$$\n",
        "\n",
        "We can find the optimal vector $\\omega$ by taking derivative with respect of $\\omega$ and setting it equal to zero.\n",
        "$$\\frac{\\partial J}{\\partial \\omega} = (Y - \\Phi\\omega)^T (-2\\Phi) + 2\\lambda \\omega^T = 0,$$\n",
        "or equivalently,\n",
        "$$-\\Phi^T(Y - \\Phi\\omega) + \\lambda \\omega = 0.$$\n",
        "Solving for $\\omega$, we get:\n",
        "$$\\omega = (\\Phi^T\\Phi+\\lambda I)^{-1}\\Phi^T Y .$$\n",
        "\n",
        "*Observation*: It is important to note that in order to obtain the optimal $\\omega$ using ridge regression, we must calculate the inverse of a $d \\times d$ matrix. \n",
        "\n",
        "As previously mentioned, there may be situations where computing the transformation $\\phi$ is not feasible due to computational expense or even impossibility. Therefore, eliminating the need for this calculation is crucial, and this is where the kernel trick can save the day.\n",
        "\n",
        "The kernel trick is based on the Mercer's Theorem, which states that a symmetric function $k(x,y)$ can be represented as an inner product $k(x,y) = <\\phi (x),\\phi (y)>$ for some function $\\phi$, if and only if $k(x,y)$ is positive semidefinite. A function that satisfies these conditions is called a kernel function. This theorem allows us to use a kernel function without explicitly computing the transformation $\\phi$, as long as the kernel function is positive semidefinite. This is a powerful tool that enables us to apply kernel methods to a wide range of problems, even when the data is high dimensional or nonlinear. \n",
        "\n",
        "It can be restated for the matrix case: A symmetric function $k(x,y)$ can be represented as an inner product $k(x,y) = <\\phi (x),\\phi (y)>$ for some function $\\phi$, if and only if, the matrix $$K=\\begin{bmatrix} k(x_1,x_1) & k(x_1,x_2) & \\cdots & k(x_1,x_n) \\\\\n",
        "k(x_2,x_1) & k(x_2,x_2) & \\cdots & k(x_2,x_n) \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "k(x_n,x_1) & k(x_n,x_2) & \\cdots & k(x_n,x_n) \\\\\n",
        "\\end{bmatrix}$$ \n",
        "is positive semidefinite for any collection $\\{x_1,x_2,\\cdots,x_n\\}$.\n",
        "So, how does this relate to our problem? Let's think about it for a second. How do the elements of $\\Phi \\Phi^T$ actually look like? It is the following $n\\times n$ matrix:\n",
        "$$\\Phi \\Phi^T = \\begin{bmatrix}\n",
        "\\phi(x_1)^T\\phi(x_1) & \\phi(x_1)^T\\phi(x_1) & \\cdots & \\phi(x_1)^T\\phi(x_1) \\\\\n",
        "\\phi(x_1)^T\\phi(x_1) & \\phi(x_1)^T\\phi(x_1) & \\cdots & \\phi(x_1)^T\\phi(x_1) \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi(x_1)^T\\phi(x_1) & \\phi(x_1)^T\\phi(x_1) & \\cdots & \\phi(x_1)^T\\phi(x_1) \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "It's almost the same as before, isn't it? Keep in mind that when $\\phi(x_i)$ can be written as a vector, $\\phi(x_i)^T\\phi(x_j)$ is just another way of writing $<\\phi(x_i),\\phi(x_j)>$. So, instead of computing the transformation function $\\phi$, we just need to find the right kernel function that fits our model.\n",
        "\n",
        "\n",
        "Hold on a second... we just derived the expression for $\\omega$, but something seems off. Notice that the equation above includes $\\Phi\\Phi^T$ when it should actually be $\\Phi^T\\Phi$. Luckily, we can use a nifty identity to fix this issue. When $(\\Phi\\Phi^T+\\lambda I)$ has an inverse, we can use the identity $U(VU + \\lambda I)^{-1} = (UV+\\lambda I)^{-1} U$ to rewrite the expression for $\\omega$:\n",
        "$$\\omega = \\Phi^T (\\Phi\\Phi^T+\\lambda I)^{-1} Y,$$ \n",
        "and then,\n",
        "$$\\omega = \\Phi^T (K+\\lambda I)^{-1} Y,$$ \n",
        "*Observations*:  The proof of the identity is pretty easy. We know that $UVU - \\lambda U$ can be factorized as $U(VU-\\lambda I)$ and $(UV-\\lambda I)U$, so both terms are equal. Now just multiply both sides by $(UV-\\lambda I)^{-1}$ on the left and by $(VU-\\lambda)^{-1}$ on the right to obtain the identity.\n",
        "\n",
        "\n",
        "Cool, but if you pay close attention, you may notice that we need to calculate $\\Phi^T$. However, there is a trick to solve this. \n",
        "For simplicity, let's write $\\omega$ as\n",
        "$$\\omega = \\Phi^T \\alpha $$, where $\\alpha= L Y$ and $L=(K+\\lambda I)^{-1}$.\n",
        "So $\\omega$ can be rewritten as:\n",
        "$$\\omega = \\sum_{i=1}^n \\alpha_i \\phi(x^{(i)}).$$\n",
        "It means that $\\omega$ is a linear combination of the $\\phi(x^{(i)})$'s.\n",
        "So, when we want to estimate a value of $y'$ from a given $x'$, we have that:\n",
        "$$ y' = \\left(\\sum_{i=1}^n \\alpha_i \\phi(x^{(i)})\\right)^T \\phi(x'),$$\n",
        "$$ y' = \\sum_{i=1}^n \\alpha_i \\phi(x^{(i)})^T \\phi(x'),$$\n",
        "and,\n",
        "$$ y' = \\sum_{i=1}^n \\alpha_i k(x^{(i)},x').$$\n",
        "\n",
        "*Observation:* Rewriting $\\omega$ using the identity seems good, but remember that now we have to compute the inverse of a $n\\times n$ matrix or solve an equivalent problem in order to find the values $\\alpha_i$. So we have to think better when $n$ is much more greater than $d$, and it is possible to calculate $\\phi(x)$.\n",
        "\n",
        "\n",
        "### Examples\n",
        "* Linear kernel: $k(x,y)=x^T y + r$.\n",
        "* Polynomial kernel: $k(x,y) = (x^Ty + r)^d$.\n",
        "* Gaussian kernel: $k(x, x') = \\exp(-\\frac{1}{2\\sigma^2} ||\\phi(x) - \\phi(x')||^2)$. It is infinite-dimensional if $\\phi$ maps x into an infinite dimensional vector space.\n",
        "* RBF kernel: $k(x, y) = e^{-\\gamma\\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert^2}.$"
      ],
      "metadata": {
        "id": "m-c71kJBMS5o"
      }
    }
  ]
}